\documentclass{article}

\usepackage[english]{babel}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{siunitx}
\usepackage{float}
\usepackage[thinc]{esdiff}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{booktabs}
\usepackage{minted}

\usepackage[pdfborderstyle={/S/U/W 0}]{hyperref}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{prop}{Proposition}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}

\section{Generating data}

\subsection{Intro}

The cost function, in function of departure time \(t_d\), is
\begin{equation}
  \label{eq:cost_td}
  C(t_d) = \alpha tt(t_d) + \beta[t^*-t_d-tt(t_d)]^+ + \gamma[t_d+tt(t_d)-t^*]^+ 
\end{equation}
where
\begin{itemize}
\item \(t^*\) is the desired arrival time
\item \(\alpha\) is the value of time spent travelling
\item \(\beta\) is the value of time spent waiting there
\item \(\gamma\) is the value of time arriving late
\item \(tt(t_d)\) is the time spent travelling if leaving at time \(t_d\)
\item \([x]^+ = \max(0, x)\)
\end{itemize}

\subsection{Are $t_a$ and $t_d$ equivalent?}

It would be helpful to express the cost function in term of the arrival time \(t_a = t_d + tt(t_d)\),
so that the second part of the cost function would greatly simplify
\[ \beta[t^*-t_d-tt(t_d)]^+ + \gamma[t_d+tt(t_d)-t^*]^+ = \beta[t^*-t_a]^+ + \gamma[t_a-t^*]^+ \]

But how do we express the first term \(tt(t_d)\) in terms of \(t_a\)?

\begin{equation}
  \label{eq:ta_td}
  t_a(t_d) = t_d + tt(t_d)
\end{equation}

Note that the travel time can be expressed in terms of the arrival time \(t_a\) if and only if the function
\eqref{eq:ta_td} is invertible:
this is because the departure time can be easily reconstructed from the travel time and the arrival time (and viceversa).

\subsubsection{When are they equivalent?}

Inverting \eqref{eq:ta_td} is not analytically possible a priori, and may not be possible in general.
It only depends on whether the function \(-tt(t_d)\) ever grows more than the identity.
For instance, if the travel time is a gausssian
\begin{equation*}
  tt(t_d) = \frac{1}{\sigma\sqrt{2\pi}}\exp\left(-{\frac{t_d^2}{2\sigma^2}}\right)
\end{equation*}
then the arrival time is invertible if and only if the variance \(\sigma\) satisfies the condition
\[\sigma \geq (2\pi e)^{-\frac{1}{4}} \approx 0.492 \]

In general, assuming that the function grows less than the identity is pretty reasonable,
since in real world I think leaving later results in arriving later.

From now on, assume \eqref{eq:ta_td} is invertible, and \(tt_a(t_a) = tt(t_d(t_a))\) exists.
Moreover, \(\alpha = 1\).

\subsection{Where is the cost minimized?}

\begin{equation}
  \label{eq:cost_ta}
  C(t_a) = tt_a(t_a) + \beta[t^*-t_a]^+ + \gamma[t_a-t^*]^+
\end{equation}

The cost function \eqref{eq:cost_ta} could be minimized either at the only non differentiable point (for \(t_a = t^*\))
or at one of the points where its derivative is zero.
\begin{equation}
  \label{eq:cost_diff}
  C'(t_a) =
  \begin{cases}
    tt_a'(t_a) -\beta \quad &\text{if } t_a < t^* \\
    tt_a'(t_a) + \gamma \quad &\text{otherwise}
  \end{cases}
\end{equation}

Setting it equal to zero, the minimum is realized by one of the points

\begin{align*}
  & t_a |\ tt_a'(t_a) = \beta, t_a < t^* \\
  & t_a = t^* \\
  & t_a |\ tt_a'(t_a) = -\gamma, t_a > t^* \\
\end{align*}

\subsubsection{If travel time is 1-lipschitz then we have at most 2 minima}

Note that, here, assuming

\begin{equation}
  \label{eq:cond_late_min}
|tt_a'(t_a)| < 1 \ \forall t_a, \qquad \gamma > \alpha = 1
\end{equation}
implies that there is no time that satisfies the last condition, that means that arriving late never realises a (not even local) minimum.

These assumptions are reasonable for what discussed earlier, but \textcolor{red}{this should be looked at better}.

\textcolor{green}{I looked at it better!} Here is the results

\subsubsection{We don't need travel time to be 1-lipschitz, in general we have 3 minima}

This is because we only need \(tt(t_d)\) to decrease slower than \(-x\), and this will make \(tt_a(t_a)\) never increase more than \(x\).

Consider indeed

\begin{align*}
  t_a(t_d) & = t_d + tt(t_d)
\end{align*}

This yields
\begin{equation*}
  tt_a(t_a) = tt(t_a^{-1}(t_a))
\end{equation*}
differentiating,
\begin{align*}
  \diffp{tt_a(t_a)}{{t_a}} & = tt'(t_a^{-1}(t_a)) \diffp{t_a^{-1}(t_a)}{{t_a}} \\
  & = tt'(t_a^{-1}(t_a)) \frac{1}{t_a'(t_a^{-1}(t_a))}
\end{align*}

But we know that \(t_a'(t_d) = 1 + tt'(t_d)\).

This implies
\begin{align*}
  \diffp{tt_a(t_a)}{{t_a}} & = tt'(t_d) \frac{1}{1+tt'(t_d)} \\
  & = \frac{tt'(t_d)}{1+tt'(t_d)} < 1
\end{align*}

\subsubsection{Gaussian travel time}

Again, let's look at the case in which the travel time is gaussian.
I assume that the travel time is gaussian in function of the arrival time as well,
and this can be justified by saying that
\[t_a - t_d >> \max_t | tt'(t)|\]
but \textcolor{red}{I doubt that this is an assumption that can be made}.
Still, assume that
\begin{equation}
  \label{eq:travel_time_gauss}
  tt_a(t_a) = \frac{1}{\sigma\sqrt{2\pi}}\exp\left(-{\frac{t_a^2}{2\sigma^2}}\right)
\end{equation}
and so
\begin{equation}
  \label{eq:travel_time_gauss_diff}
  tt_a'(t_a) = -\frac{t_a}{\sigma^3\sqrt{2\pi}}\exp\left(-{\frac{t_a^2}{2\sigma^2}}\right)
\end{equation}
This can't be inverted analytically and the solutions to the equations above can be found numerically.
(I hope other approaches for minimizing the cost are possible, but nothing comes to my mind right now).

\subsubsection{In general, how to find the minima}


In order to find the minimum of the function \(C(t_a)\) there are thus two possible procedures (that are actually the same one...).
\begin{itemize}
\item Running a root finding algorithm on the functions \(tt_a'(t_a) - \beta,\ tt_a'(t_a) + \gamma\) to find at most two local minima, to be compared to the one potentially realized by \(t^*\) to find the minimum
\item Directly running an optimizer on the cost function \(C(t_a)\). This would find one of the local minima.
  Initializing two optimizers for a very high value and a very low value would find the two minima found above.
  Moreover, in the case in which conditions \eqref{eq:cond_late_min} hold,
  it is enough to launch one optimizer for a very little value.
  This could be the best way, since it is simpler and equally effective (and potentially delegates the computation of an explicit expression for \(tt'\) to an automatic differentiation framework).
\end{itemize}

I will thus implement the second point.

\textcolor{blue}{The second point is not working too well due to (appearently) some problems with jaxopt.GradientDescent. Maybe implementing the first one is actually not a bad idea.}

\section{Is the optimum monotonous in $t^*$?}

\subsection{If travel time is 1-Lipschitz}

In particular, let's again assume the travel time to be 1-Lipschitz (so that the minimum of the cost function can never be realized by \(t > t^*\)), and \(C^1(\mathbb{R})\)
on top of going asymptotically to zero as the arrival time goes to \(\infty\) and \(-\infty\).

As discussed earlier, the cost will be minimized either for a point for which \(tt_a'(t_a) = \beta\) or for \(t_a = t^*\).

Consider thus the sets
\begin{align*}
  B & = \{t_a | tt_a'(t_a) = \beta, tt_a'' > 0\} \\
  B_{t^*}&  = \{t_a | tt_a'(t_a) = \beta, t_a \leq t^*\}
\end{align*}

\subsubsection{Where the travel time function is increasing}

First of all, consider the intervals in which the travel time function \(tt_a\) is increasing.

Here, new potential optima can be created.
The following lemma shows that every time a potential optimum is created (when the set \(B_{t^*} \) grows), the function will locally be constant.

\begin{lemma}
  Let \(t_b \in B\). Then, \(\exists \delta > 0 |\ t_a(t^*) = t_b \ \forall t^* \in [t_b, t_b + \delta)\)
\end{lemma}
\begin{proof}
  Let \(t_0 = t_b + \epsilon\), such that \(tt'(t) > \beta\ \forall\ t \in (t_b, t_0)\).

  The function to be minimized will be
  \begin{equation*}
    C[t^* = t_0](t_a) = tt_a(t_a) + \beta(t_0 - t_a)
  \end{equation*}
  and will be minimized either for \(t_a = t_0\) or for \(t_a = t_b\).

  But, since the derivative is lower bounded,

  \begin{align*}
    C[t^*=t_0](t_0) & = tt_a(t_0) \\
    & = tt_a(t_b + \epsilon) \\
    & = tt_a(t_b) + \int_{t_b}^{t_0} tt'(t) dt \\
    & \geq tt_a(t_b) + \beta(t_0 - t_b) \\
    & = C[t^* = t_0](t_b)
  \end{align*}

  The minimum is thus constant for each admissible value of \(\epsilon\), and such an epsilon can be chosen by continuity of the function \(tt_a'\).
\end{proof}

The following lemma shows that the function can never \textit{jump back}:

\begin{lemma}
  Let \(t_b \in B\), \(t_0 > t_b\) such that \(t_a(t_0) = t_0\).
  
  Then, \(\nexists t_1 > t_0 |\ t_a(t_1) = t_b\),
\end{lemma}
\begin{proof}
  There are two cases: either \(tt_a'(t_0) \geq \beta\) or \(tt_a'(t_0) < \beta\).

  If  \(tt_a'(t_0) < \beta\), then the minimum realized  by \(t_0\) is growing slower than the minimum realized by \(t_a\): we can reduce \(t_0\), until we fall in the other case. We must fall in the other case, since \(t_b \in B\)

  If \(tt_a'(t_0) \geq \beta\), it is impossible that the hypothesis \(t_a(t_0) = t_0\) is satisfied:
  each time that \(tt_a'(t_0) \geq \beta\), for what said earlier the function is constant, and not linear.
\end{proof}

As long as the function \(tt\) is increasing, the only admissible jumps are upwards (from a constant function to the identity).

\subsubsection{Where the travel time function is decreasing}

Where the function is decreasing, no new candidates for the minima are possible, since \(\beta > 0\).

If \(t_a(t) = t\), then as long as the function \(tt\) is decreasing the optimal time will grow linearly.

Otherwise, if \(t_a(t) = t_b \neq t\), then the function will remain constant until \(tt_a(t) = \beta (t - t_b)\),
and then jump and grow, again, as the identity.

\subsection{In general}

Consider now an arbitrary travel time function \(tt_a(t_a)\).

Consider the sets

\begin{align*}
  B & = \{t_a | tt_a'(t_a) = \beta, tt_a'' > 0\} \\
  G & = \{t_a | tt_a'(t_a) = -\gamma, tt_a'' > 0\} \\
\end{align*}

The function \(t_a(t^*)\) is (locally) either equal to the identity \(t_a(t^*) = t^*\) or constant, assuming a value that is in one of the defined sets \(B, G\)

\begin{prop}
  Let \(tt_a(t_a)\) be a travel time function with only one local maximum \(\hat{t}\),
  and let (in increasing order) \(B = \{b_1, \dots, b_n\}\), \(G = \{g_1, \dots, g_m\}\).


  Consider the collections of intervals \(B^* = \{(b_i, f(b_i))\}_i\) and \(G^* = \{(h(g_i), g_i)\}_i\), where
  
  \begin{align*}
    f(t) & = \min\{x>t | \int_t^x(tt_a(s) - tt_a(t)) ds = 0\} \\
    h(t) & = \max\{x<t | \int_x^t(tt_a(s) + tt_a(t)) ds = 0\} \\
  \end{align*}

  If \(f(b_n) \leq g(g_1)\), then the function \(t_a(t^*)\) will be as follows:

  \begin{align*}
    t_a(t^*) = 
    \begin{cases}
      \min \{b_i | t^* \in (b_i, f(b_i)\} & \text{if } \exists i |\ b_i < t^* < f(b_i) \\
      \max \{g_i | t^* \in (h(g_i), g_i\} & \text{if } \exists i |\ h(g_i) < t^* < g_i \\
      t^* & \text{otherwise}
    \end{cases}
  \end{align*}

  Otherwise, if \(f(b_n) > g(g_1)\), for \(f(b_n) < t^* < h(g_1)\) the function will be made by an increasing succession of constant zones.
\end{prop}

\section{Retrieving parameters}

\subsection{Intro}

For retrieving the parameters from the data, I will try to define a function that,
given the distribution of the parameters, estimates the likelihood of each point.

For this, it is probably a good idea to use the characterization of the function \(t_a(t^*)\) above.

From now on,
\begin{equation*}
  ot(t) := t_a(t)
\end{equation*}

Let \(t_a\) be a sample of the optimal arrival time. The likelihood of the parameters will be
\begin{equation}
  \label{eq:likelihood_def}
  \mathcal{L}(\mu_\beta, \mu_\gamma, \mu_t, \sigma, \sigma_t\ \vert\ T_a = t_a) =
  f_{T_a; \mu_\beta, \mu_\gamma, \mu_t, \sigma, \sigma_t}(t_a)
\end{equation}

The random variable \(T_a\) is the core of the method: it takes, as parameters, mean and variances of normally distributed \(\beta, \gamma, t^*\) and represent the resulting distribution for the point \(ot(t^*)\) that minimizes the cost.
\(f_{T_a; \theta}\) is its probability density function, given the parameters \(\theta\).

Let's thus study the likelhook in \eqref{eq:likelihood_def}.
Given a point \(t_a\), it is either an internal minimum or a kink minimum (and thus equal to \(t^*\)).

Hence, let \(Q\) be a binary random variable, defined so that \(Q=1\) if a point is an internal minimum, \(Q=0\) otherwise.

The pdf can be decomposed in

\begin{equation}
  \label{eq:likelihood_split}
  f_{T_a}(t_a) = f_{T_a | Q}(t_a | 0)\mathbb{P}(Q = 0) + f_{T_a | Q}(t_a | 1)\mathbb{P}(Q = 1)
\end{equation}

From now on, for simplicity we suppose there is at most one point which realizes
\[tt'(t_\beta) = \beta, tt''(t_\beta) > 0\]
and at most one that satisfies
\[tt'(t_\gamma) = -\gamma, tt''(t_\gamma) > 0\]
for any choice of \(\beta\) and \(\gamma\).

Let then
\begin{align*}
  b_i & : \beta \mapsto t_\beta \\
  b_e & : \beta \mapsto t | tt(t) = \beta (t - t_\beta) + tt(t_\beta), t > t_\beta\\
  g_i & : \gamma \mapsto t | tt(t) = \gamma (t_\gamma - t) + tt(t_\gamma), t < t_\gamma\\
  g_e & : \gamma \mapsto t_\gamma
\end{align*}

Note that \(b_e\) and \(g_e\) are well defined, since if there were 2 points that satisfy the condition then our assumption of having a unique \(t_\beta\) and \(t_\gamma\) would fail.

These functions are important because they define the mapping \(ot: t^* \mapsto t_a\).
On top of the assumption already made, assume that \(g_i(\gamma) > b_e(\beta)\). \textcolor{red}{This assumption is not reaslistic and should not be made, but it's probably pretty easy to get rid of}.

We now have a complete characterization of the function \(ot\):

\begin{equation}
  \label{eq:characterized_ot}
  ot(t^*) =
  \begin{cases}
    b_i(\beta) & \text{if } t^* \in (b_i(\beta), b_e(\beta)) \\
    g_e(\gamma) & \text{if } t^* \in (g_i(\gamma), g_e(\gamma)) \\
    t^* & \text{otherwise}
  \end{cases}
\end{equation}

We can thus see a bit better what equation \eqref{eq:likelihood_split} means, since we know where the internal minima are and where the kink actually minimizes the cost function.

\subsection{Probability of the point being a kink minimum}

The event \textit{being a kink minimum} can thus be decomposed in \textit{being a realization of the random variable \(T^*\)} and \textit{not being into an interval that would yield an internal minimum}.

These events are independent, since \(T^*\) is independent from the variables \(\gamma\) and \(\beta\) that decide the intervals.

The pdf for the variable \(K\) will thus be

\begin{equation}
  \label{eq:prob_kink}
    f_{T_a | Q}(t_a | 0) = \frac{f_{T^*}(t_a)\mathbb{P}( t_a \not\in (b_i(\beta), b_e(\beta)) \cup (g_i(\gamma), g_e(\gamma)))}{\int_{-\infty}^\infty f_{T^*}(t_a)\mathbb{P}( t_a \not\in (b_i(\beta), b_e(\beta)) \cup (g_i(\gamma), g_e(\gamma)))dt_a}
  \end{equation}
 where the denominator is nothing but a normalization constant, found by integrating the numerator over all the domain of \(t_a\).

\(f_{T^*}(t_a)\) is known, since the distribution of the variable \(T^*\) is known.

For the probability of not being in the interval, on the other hand, we have 

\begin{equation*}
  \mathbb{P}( t_a \not\in (b_i(\beta), b_e(\beta)) \cup (g_i(\gamma), g_e(\gamma)))dt_a = \int_{b\in \mathbb{R} \vert t_a \not\in (b_i(b), b_e(b))}\int_{g \in \mathbb{R} \vert t_a \not\in (g_i(g), g_e(g))}f_\beta(b)f_\gamma(g)\, dg\, db
\end{equation*}

This is pretty straightforward, except for a problem in defining the domain of the integral, which will be numerically estimated.

The following proposition shows that the intervals shrink with increasing \(\beta\) (and with decreasing \(\gamma\)), making the estimation of the domain
\begin{prop}
  \(b_i(\beta)\) is increasing in \(\beta\), while \(b_e(\beta)\) is decreasing in \(\beta\).
  Similarly, \(g_i\) is increasing and \(g_e\) is decreasing.
\end{prop}
\begin{proof}
  This simply follows from the convexity of the function in \(t_\beta, t_\gamma\), and from the assumption of having only one point with the given slope (that implies that in \(b_e(\beta), g_i(\gamma)\) the function is concave).
\end{proof}
This considerably simplifies the approximation of the integration domain:
we indeed now have that there exist \(\beta_0(t_a), \gamma_0(t_a) \in \mathbb{R}\) such that
\begin{align}
  \label{eq:threshold_integration}
  \begin{split}
     t_a \not\in (b_i(b), b_e(b)) \iff b > \beta_0(t_a) \\
    t_a \not\in (g_i(g), g_e(g)) \iff g > \gamma_0(t_a)
  \end{split}
\end{align}

Let thus
\begin{align*}
  \beta_\text{max} & = \max_x{tt'(x)} \\
  \gamma_\text{max} & = \max_x{-tt'(x)} \\
\end{align*}

Since the values of \(\beta_0, \gamma_0\) can be found by bisection (this because the functions \(b_i, b_e, g_i, g_e\) can be numerically evaluated as well), by initializing \(\beta\) to \((0, \beta_\text{max})\), \(\gamma\) to \((1, \gamma_\text{max})\), the expression in \eqref{eq:prob_kink} becomes

\begin{align*}
  f_{T_a | Q}(t_a | 0) & = \frac{f_{T^*}(t_a)\int_{\beta_0(t_a)}^\infty f_\beta(b)\, db\int_{\gamma_0(t_a)}^\infty f_\gamma(g)\, dg}{\int_{-\infty}^\infty f_{T^*}(t_a)\int_{\beta_0(t_a)}^\infty f_\beta(b)\, db\int_{\gamma_0(t_a)}^\infty f_\gamma(g)\, dgdt_a} \\
  & = \frac{f_{T^*}(t_a)\int_{\beta_0(t_a)}^{\beta_\text{max}}f_\beta(b)\, db\int_{\gamma_0(t_a)}^{\gamma_\text{max}}f_\gamma(g)\, dg}{\int_{-\infty}^\infty f_{T^*}(t_a)\int_{\beta_0(t_a)}^{\beta_\text{max}}f_\beta(b)\, db\int_{\gamma_0(t_a)}^{\gamma_\text{max}}f_\gamma(g)\, dgdt_a}
\end{align*}

Note that the coefficients \(\beta_\text{max}\), \(\gamma_\text{max}\) depend only on the travel time function and can be computed only once per travel time function.

Moreover, the denominator is exactly \(\mathbb{P}(Q = 0)\).
It will thus simplify in \eqref{eq:likelihood_split}

\subsection{Probability of the point being an internal minimum}

I will thus here concentrate on the function \(f_I(t_a) = f_{T_a | Q}(t_a | 1)\).

Again, the probability density function can be split in 2 parts:
one regarding the \textit{early} minima and one regarding the \textit{late} minima.

Let's thus define again a binary random variable \(L\), where \(L=1\) for the event \textit{arriving late}, \(L=0\) for \textit{arriving early}.

\begin{equation}
  \label{eq:internal_split}
  f_I(t_a) = f_{I | L}(t_a | 0) \mathbb{P}(L=0 | Q=1) + f_{I | L}(t_a | 1) \mathbb{P}(L=1 | Q=1)
\end{equation}

\(f_{I | L}(t_a | 0)\) is the distribution arising from applying the transformation \(b_i\) to a variable distributed as \(\beta\) (similarly for \(\gamma\)).

We have
\begin{align}
  \label{eq:pdf_bi}
  \begin{split}
    \beta & \sim \mathcal{N}(\mu_\beta, \sigma_\beta) \\[1em]
    f_{I | L}(t_a | 0) & = \text{pdf}(b_i(\beta))(t_a) \\
    & = f_\beta(b_i^{-1}(t_a))\diff{b_i^{-1}}{t_a}(t_a)
  \end{split}
\end{align}

Note now that computing the inverse of \(b_i\) is exactly just looking at the steepness of the travel time function, where the travel time function is convex.

The probability density function becomes thus

\begin{equation}
  \label{eq:pdf_early}
  f_{I | L}(t_a | 0) = f_\beta(tt_a'(t_a))[tt_a''(t_a)]^+
\end{equation}
and, analogously,

\begin{equation}
  \label{eq:pdf_late}
  f_{I | L}(t_a | 1) = f_\gamma(-tt_a'(t_a))[tt_a''(t_a)]^+
\end{equation}

The only thing that is left is now to compute the probabilities \(\mathbb{P}(L=1 | Q=1)\), \(\mathbb{P}(L=0 | Q=1)\).

As it turns out, it is easier to just compute the joint probability \(\mathbb{P}(L, Q\) instead of the conditional one, and the conditional one will then be simplified when substituting in \eqref{eq:likelihood_split}.

The probability \(\mathbb{P}(L=0, Q=1)\) will be exactly the probability that \(t^*\) is in the interval \((b_i(b), b_e(b))\), for some realization of the variable \(\beta\ b\).

It will thus be sufficient to integrate twice the pdf:

\begin{align}
  \label{eq:prob_int_beta}
  \begin{split}
    \mathbb{P}(L=0, Q=1) & =\mathbb{P}(t^* \in (b_i(\beta), b_e(\beta))) \\
    & = \int_0^{\beta_\text{max}}\int_{b_i(b)}^{b_e(b)} f_{T^*}(t)dtf_\beta(b)db
      \end{split}
\end{align}

and similarly for \(\gamma\),
\begin{equation}
  \label{eq:prob_in_gamma}
  \mathbb{P}(L=1, Q=1) = \int_1^{\gamma_\text{max}}\int_{g_i(g)}^{g_e(g)} f_{T^*}(t)dtf_\gamma(g)dg
\end{equation}

\end{document}
%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: t
%%% End:
